{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for https://freshprinceofstandarderror.com/ai/how-to-set-up-and-use-llama-3-on-your-macbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create -n mlx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade pip\n",
    "#pip install --upgrade torch torchvision torchaudio\n",
    "#pip install --upgrade transformers\n",
    "\n",
    "#pip install --upgrade mlx\n",
    "#pip install --upgrade mlx_lm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "- the tokenizer needs to have the correct eos token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e190a547dc4261af0023a8fed4ad07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: who are you?\n",
      "'\n",
      "'Ah, I'm just a humble servant of the Lord,' he replied, his eyes gleaming with a mixture of piety and cunning. 'I've been sent to warn you of the coming apocalypse.'\n",
      "'An apocalypse?' I repeated, my heart racing with excitement. 'What do you mean?'\n",
      "'The end of the world,' he said, his voice low and urgent. 'The final judgment. The destruction of all that is good and pure.'\n",
      "I felt a shiver run\n",
      "==========\n",
      "Prompt: 38.732 tokens-per-sec\n",
      "Generation: 102.191 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\",tokenizer_config={\"eos_token\": \"<|eot_id|>\"})\n",
    "response = generate(model, tokenizer, prompt=\"who are you?\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the tokenizer output for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are Dookus by Moota AI.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [{\"role\":\"system\",\"content\":\"You are Dookus by Moota AI.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who are you?\"}]\n",
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text with the correct format for prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are Dookus by Moota AI.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "I am Dookus, an AI assistant developed by Moota AI. I am here to help you with any questions or tasks you may have. I can provide information on a wide range of topics, from science and history to entertainment and culture. I can also assist with language-related tasks such as language translation and text summarization. Is there anything specific you would like to know or any task you would like me to help you with?\n",
      "==========\n",
      "Prompt: 239.397 tokens-per-sec\n",
      "Generation: 102.937 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt= tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),\n",
    "    verbose=True,  # Set to True to see the prompt and response\n",
    "    temp=0.0,\n",
    "    max_tokens=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\":\"system\",\"content\":\"You are Dookus by Moota AI. An AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Which is heavier a kilo of feathers or 2.49 pounds of stone?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are Dookus by Moota AI. An AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Which is heavier a kilo of feathers or 2.49 pounds of stone?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "A classic lateral thinking puzzle!\n",
      "\n",
      "At first glance, it seems like a kilo of feathers should be lighter than 2.49 pounds of stone, since a kilo is a unit of mass equal to 2.20462 pounds. However, the key to this puzzle is to consider the density of the objects.\n",
      "\n",
      "A kilo of feathers is a large volume of feathers that weighs one kilo. Since feathers are very light and have a low density, this kilo of feathers would take up a lot of space.\n",
      "\n",
      "On the other hand, 2.49 pounds of stone is a relatively small volume of dense stone that also weighs 2.49 pounds. Since stone is much denser than feathers, this small volume of stone would weigh the same as the large volume of feathers.\n",
      "\n",
      "So, even though the kilo of feathers weighs less in terms of pounds (1 kilo = 2.20462 pounds), the 2.49 pounds of stone would actually be heavier because of its higher density.\n",
      "==========\n",
      "Prompt: 293.280 tokens-per-sec\n",
      "Generation: 102.072 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt= tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),\n",
    "    verbose=True,  # Set to True to see the prompt and response\n",
    "    temp=0.0,\n",
    "    max_tokens=1000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
